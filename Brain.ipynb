{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from othello import Board\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "#Define print de matrizes inteiras no numpy\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "class Statistics:\n",
    "    def __init__(self, tensorboard_file):\n",
    "        self.tensorboard_file = tensorboard_file\n",
    "    \n",
    "    #Salva dados de variaveis para o tensorboard \n",
    "    def add_data(self, data):\n",
    "        data = data if type(data) == list or type(data) == np.ndarray else [data]\n",
    "        for var in data:\n",
    "            #Calcula media\n",
    "            mean = tf.reduce_mean(var)\n",
    "            #Salva para tensorboard\n",
    "            tf.summary.scalar('mean', mean)        \n",
    "            #Calcula desvio padrão\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            #Salva desvio padrão para tensorboard\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            #Salva maximo para tensorboard\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            #Salva minimo para tensorboard\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            #Salva histogram para tensorboard\n",
    "            tf.summary.histogram('histogram', var)\n",
    "            \n",
    "    def start(self):\n",
    "        #Inicializa e configura tensorBoard\n",
    "        self.merged_summary = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter(self.tensorboard_file)\n",
    "\n",
    "class Game:    \n",
    "    def __init__(self):\n",
    "        self.board = Board()\n",
    "        self.BLACK = 1\n",
    "        self.WHITE = -1\n",
    "           \n",
    "    #Efetua a proxima action com o agente\n",
    "    def player_move(self, brain, player):\n",
    "        #Define o board de acordo com o player da vez\n",
    "        temp_board = (self.board.board * player).reshape([1,-1])\n",
    "\n",
    "        #Recebe do board todos os movimentos permitidos ao player atual\n",
    "        moves = self.board.possible_moves(player) \n",
    "        \n",
    "        #Se não existir nenhuma ação que possa ser tomada\n",
    "        if(len(moves) == 0):\n",
    "            #retorna avisando que o player não pode se mover\n",
    "            return -1\n",
    "        \n",
    "        #Calcula probabilidades baseadas nas possiveis rewards futuras para definir movimento a ser tomado\n",
    "        probs = brain.inference(temp_board)\n",
    "\n",
    "        #Calcula rewards para todos os movimentos\n",
    "        new_probs = np.zeros(brain.INPUT_DIM)\n",
    "        \n",
    "        #Monta lista de probabilidades para todos os movimentos possiveis\n",
    "        for x, y, _ in moves:\n",
    "            new_probs[x*8 + y] = probs[x*8 + y]\n",
    "\n",
    "        #Se a rede achar que nao deve tomar nenhuma ação\n",
    "        if(np.sum(new_probs) == 0):\n",
    "            #Sorteia um movimento a ser tomado\n",
    "            selected_move = np.random.choice(len(moves), 1, p = [1/len(moves)]*len(moves))[0]\n",
    "            selected_move = moves[selected_move][0]*8 + moves[selected_move][1]\n",
    "        else: #Caso existam movimentos com probabilidades de serem tomadas\n",
    "            #Sorteia um movimento baseado nas probabilidades\n",
    "            new_probs = new_probs/np.sum(new_probs)\n",
    "            selected_move = np.random.choice(brain.INPUT_DIM, 1, p=new_probs)[0]\n",
    "\n",
    "        #Efetua movimento no board\n",
    "        self.board.move(selected_move//8, selected_move%8, player)\n",
    "        #retona movimento tomado\n",
    "        return selected_move\n",
    "    \n",
    "    \n",
    "    #Simula um jogo inteiro e retorna (Boards, actions, rewards) de todo o jogo para ambos os players (Rede X Rede)\n",
    "    def play(self, brain):\n",
    "        #Inicia um novo board\n",
    "        self.board.reset()\n",
    "\n",
    "        #Define listas de States e Actions\n",
    "        board_history_p1 = []\n",
    "        board_history_p2 = []\n",
    "        move_sequence_p1 = []\n",
    "        move_sequence_p2 = []\n",
    "\n",
    "        #Printa estado inicial se verbose = true\n",
    "        brain.v or print(self.board)\n",
    "        #Toma jogadas ate que o jogo termine\n",
    "        while(not self.board.finished()):\n",
    "            #Guarda estado anterior\n",
    "            board_before_move_p1 = deepcopy(self.board.board)\n",
    "            #Faz melhor movimento\n",
    "            p1_move = self.player_move(brain, self.BLACK)\n",
    "            #Se o player pode se mover\n",
    "            if(p1_move != -1):\n",
    "                #Registra estado\n",
    "                board_history_p1.append(board_before_move_p1)\n",
    "                #Registra action\n",
    "                move_sequence_p1.append(p1_move)\n",
    "\n",
    "            #Guarda estado anterior\n",
    "            board_before_move_p2 = deepcopy(self.board.inverted_board())\n",
    "            #Faz melhor movimento\n",
    "            p2_move = self.player_move(brain, self.WHITE)\n",
    "            #Se o player pode se movere\n",
    "            if(p2_move != -1):\n",
    "                #Registra estado\n",
    "                board_history_p2.append(board_before_move_p2)\n",
    "                #Registra action\n",
    "                move_sequence_p2.append(p2_move)\n",
    "                \n",
    "            #Printa round status se verbosee = true\n",
    "            brain.v or print(\"====================\")\n",
    "            brain.v or print((move_sequence_p1[-1]//8, move_sequence_p1[-1]%8))\n",
    "            brain.v or print(self.board)\n",
    "            brain.v or print(\"====================\")\n",
    "            brain.v or print((move_sequence_p2[-1]//8, move_sequence_p2[-1]%8))\n",
    "            brain.v or print(self.board)\n",
    "        brain.v or print(self.board.score())\n",
    "\n",
    "        #Calcula reward para todas as ações de BLACK\n",
    "        reward_sequence_p1 = brain.calculate_reward(self.BLACK, move_sequence_p1)\n",
    "        #Calcula reward para todas as ações de WHITE\n",
    "        reward_sequence_p2 = brain.calculate_reward(self.WHITE, move_sequence_p2)\n",
    "\n",
    "        #Concatena todos os estados de BLACK e WHITE\n",
    "        board_history = board_history_p1 + board_history_p2\n",
    "        #Concatena todas as actions de BLACK e WHITE\n",
    "        move_sequence = move_sequence_p1 + move_sequence_p2\n",
    "        #Concatena todas as rewards de BLACK e WHITE\n",
    "        reward_sequence = reward_sequence_p1 + reward_sequence_p2\n",
    "\n",
    "        #retorna estados, actions e rewards do jogo atual\n",
    "        return (board_history, move_sequence, reward_sequence) \n",
    "    \n",
    "    #Toma uma jogada aleatoria\n",
    "    def random_move(self, player):\n",
    "        #Recebe movimentos validos do board\n",
    "        moves = self.board.possible_moves(player) \n",
    "\n",
    "        #Se não existir um movimento possivel retorna -1\n",
    "        if(len(moves) == 0):\n",
    "            return -1\n",
    "\n",
    "        #Seleciona aleatoriamente um movimento\n",
    "        selected_move = np.random.choice(len(moves), 1, p = [1/len(moves)]*len(moves))[0]\n",
    "        #Efetua movimento no tabuleiro\n",
    "        self.board.move(moves[selected_move][0], moves[selected_move][1], player)         \n",
    "        \n",
    "        \n",
    "class Brain:\n",
    "    def __init__(self, data_type = tf.float32, restore_file = \"\", tensorboard_file = './data', learning_rate = 0.01, batch_size = 64, epochCount = 100000, replay_memory_limit = 10000, gamma = 0.95, h_layers_dim = [128, 256, 128], eval_test_size = 100, gpu_num = 1, dynamic_allocation = True, verbose = False):\n",
    "        self.reset_graph()\n",
    "        self.data_type = data_type\n",
    "        self.restore_file = restore_file\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_count = epochCount\n",
    "        self.replay_memory_limit = replay_memory_limit\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.INPUT_DIM = 8*8\n",
    "        self.statistics = Statistics(tensorboard_file)\n",
    "        self.graph = self.build_graph(h_layers_dim)\n",
    "        self.replay_memory = {\n",
    "            \"states\": [],\n",
    "            \"actions\": [],\n",
    "            \"rewards\": []\n",
    "        }\n",
    "        self.v = not verbose\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = dynamic_allocation\n",
    "        self.gpu_num = gpu_num #UNUSED\n",
    "                    \n",
    "        self.sess = tf.Session(config=config)\n",
    "        \n",
    "        #Instancia um Saver usado para salvar e recuperar os pesos\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.game = Game()\n",
    "        \n",
    "        self.eval_test_size = eval_test_size\n",
    "\n",
    "\n",
    "    #Reseta computational graph\n",
    "    def reset_graph(self):\n",
    "        #Verifica se existe uma sessão aberta\n",
    "        if(\"sess\" in globals() and sess):\n",
    "            #Fecha a sessão\n",
    "            self.sess.close()\n",
    "        #Destroi o grafo remanescente\n",
    "        tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "    #Constroi computational graph\n",
    "    def build_graph(self, h_layers_dim):\n",
    "        with tf.variable_scope(\"Place_holders\"):\n",
    "            #Cria placeholder para receber o Estado Atual [batch, board]\n",
    "            input_states = tf.placeholder(self.data_type, [None, self.INPUT_DIM])\n",
    "            #Cria placeholder para receber as actions [actionNum]\n",
    "            actions = tf.placeholder(tf.int32, [None])\n",
    "            #Cria placeholder para receber as rewards [RewardVal]\n",
    "            rewards = tf.placeholder(self.data_type, [None])\n",
    "\n",
    "        #Array de Pesos\n",
    "        weights = []\n",
    "        #Array de Biases\n",
    "        biases = []\n",
    "\n",
    "        with tf.variable_scope(\"Weights\"):\n",
    "            #Cria Primeira camada (camada q recebe input)\n",
    "            weights.append(tf.get_variable(\"W0\", [self.INPUT_DIM, h_layers_dim[0]], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            #Cria Segunda camada (camada q recebe dados da camada[0])\n",
    "            weights.append(tf.get_variable(\"W1\", [h_layers_dim[0], h_layers_dim[1]], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            #Cria Terceira camada (camada q recebe dados da camada[1])\n",
    "            weights.append(tf.get_variable(\"W2\", [h_layers_dim[1], h_layers_dim[2]], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            #Cria Quarta camada (camada de output) (camada q recebe dados da camada[2])\n",
    "            weights.append(tf.get_variable(\"W3\", [h_layers_dim[2], self.INPUT_DIM], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "\n",
    "            #Calcula dados e adiciona ao tensorboard\n",
    "            self.statistics.add_data(weights)\n",
    "\n",
    "        with tf.variable_scope(\"Biases\"):\n",
    "            #Cria Bias da camada [0]\n",
    "            biases.append(tf.get_variable(\"b0\", [h_layers_dim[0]], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            #Cria Bias da camada [1]\n",
    "            biases.append(tf.get_variable(\"b1\", [h_layers_dim[1]], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            #Cria Bias da camada [2]\n",
    "            biases.append(tf.get_variable(\"b2\", [h_layers_dim[2]], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            #Cria Bias da camada [3]\n",
    "            biases.append(tf.get_variable(\"b3\", [self.INPUT_DIM], self.data_type, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "\n",
    "            #Calcula dados e adiciona ao tensorboard\n",
    "            self.statistics.add_data(biases)\n",
    "\n",
    "        #Array que armazena conexões\n",
    "        connected_layers = []           \n",
    "            \n",
    "        with tf.variable_scope(\"Connect\"):\n",
    "            connected_layers.append(tf.matmul(input_states, weights[0]) + biases[0])\n",
    "            connected_layers.append(tf.matmul(connected_layers[-1], weights[1]) + biases[1])\n",
    "            connected_layers.append(tf.matmul(connected_layers[-1], weights[2]) + biases[2])\n",
    "            connected_layers.append(tf.matmul(connected_layers[-1], weights[3]) + biases[3])\n",
    "\n",
    "            #Calcula dados e adiciona ao tensorboard\n",
    "            self.statistics.add_data(connected_layers)\n",
    "\n",
    "        #Array que armazena ativações\n",
    "        hidden_states = []           \n",
    "            \n",
    "        with tf.variable_scope(\"Activation\"):\n",
    "            for c_layer in connected_layers[:-1]:\n",
    "                #Conecta o imput a camada 0 e aplica Relu\n",
    "                hidden_states.append(tf.nn.relu(c_layer)) #Olhar o relu6, pode ser melhor\n",
    "                #adiciona dados ao histogram do tensorboard\n",
    "                tf.summary.histogram('hiddenStates', hidden_states[-1])\n",
    "\n",
    "            #Conecta a camada 2 a camada 3 e gera output\n",
    "            hidden_states.append(connected_layers[-1])\n",
    "            #adiciona dados ao histogram do tensorboard\n",
    "            tf.summary.histogram('hiddenStates', hidden_states[-1])\n",
    "\n",
    "        \n",
    "        with tf.variable_scope(\"Output\"):\n",
    "            #Output\n",
    "            unnormalized_log_probs = hidden_states[-1]\n",
    "            #Output com softmax, usado para determinar a ação\n",
    "            action_probs = tf.nn.softmax(unnormalized_log_probs)\n",
    "\n",
    "        with tf.variable_scope(\"Cross_entropy\"):\n",
    "            #Calcula cross entropy negativa para maximizar o reward\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=unnormalized_log_probs, labels=actions)\n",
    "            #Calcula loss\n",
    "            loss = tf.reduce_mean(neg_log_prob * rewards)  # reward guided loss\n",
    "            #adiciona dados ao tensorboard            \n",
    "            tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimize\"):\n",
    "            #Faz gradient descent e otimiza o modelo\n",
    "            train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "\n",
    "        with tf.variable_scope(\"Eval\"):\n",
    "            eval_player = tf.placeholder(tf.int32, [1])\n",
    "            tf.summary.scalar('Eval', eval_player[0])            \n",
    "            \n",
    "        return {\n",
    "            \"input_states\": input_states,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"eval\": eval_player,\n",
    "            \"action_probs\": action_probs,\n",
    "            \"train_step\": train_step\n",
    "        }\n",
    "    \n",
    "    #Calcula reward para cada action em um determinado estado\n",
    "    def inference(self, input_state):\n",
    "        #Usa rede para calcular quais actions são melhores em determinado estado\n",
    "        return (self.sess.run(self.graph[\"action_probs\"], feed_dict={self.graph[\"input_states\"]: input_state})).squeeze()\n",
    "\n",
    "    #Sorteia cenarios do historico de todos os (Estados, Ações, Rewards) para evitar overfiting por corelação temporal\n",
    "    def sample_batch(self):\n",
    "        #Cria array dos indices das jogadas selecionadas\n",
    "        indexes = []\n",
    "        #sorteia size jogadas distintas e captura os indices\n",
    "        while(len(indexes) < self.batch_size):\n",
    "            i = np.random.randint(0,len(self.replay_memory[\"states\"]))\n",
    "            if i not in indexes:\n",
    "                indexes.append(i)\n",
    "        #monta Lista de estados das jogadas selecionadas\n",
    "        boards = [self.replay_memory[\"states\"][i]for i in indexes]\n",
    "        #monta Lista de ações das jogadas selecionadas\n",
    "        actions = [self.replay_memory[\"actions\"][i]for i in indexes]\n",
    "        #monta Lista de rewards das jogadas selecionadas\n",
    "        rewards = [self.replay_memory[\"rewards\"][i]for i in indexes]\n",
    "        \n",
    "         #Retorna o batch de jogadas\n",
    "        return np.array(boards).reshape((self.batch_size, self.INPUT_DIM)), np.array(actions), np.array(rewards)\n",
    "    \n",
    "    def add_samples_to_replay_mem(self):\n",
    "        #Gera jogadas suficientes para compor o batch\n",
    "        replay_memory_gen_count = 0\n",
    "        while(replay_memory_gen_count <= self.batch_size):\n",
    "            #Gera jogadas fazendo um jogo de rede X rede\n",
    "            boards, moves, rewards = self.game.play(self)\n",
    "\n",
    "            #Adiciona as jogadas ao replayMemory\n",
    "            self.replay_memory[\"states\"].extend(boards)\n",
    "            self.replay_memory[\"actions\"].extend(moves)\n",
    "            self.replay_memory[\"rewards\"].extend(rewards)\n",
    "            \n",
    "            #Update count\n",
    "            replay_memory_gen_count += len(boards)\n",
    "\n",
    "        #Remove itens excedentes da replayMemory se ela estiver maior que o limite\n",
    "        if(len(self.replay_memory[\"states\"]) >= self.replay_memory_limit):\n",
    "            self.replay_memory[\"states\"] = self.replay_memory[\"states\"][len(boards)-1:]\n",
    "            self.replay_memory[\"actions\"] = self.replay_memory[\"actions\"][len(moves)-1:]\n",
    "            self.replay_memory[\"rewards\"] = self.replay_memory[\"rewards\"][len(rewards)-1:]\n",
    "    \n",
    "    def calculate_reward(self, player, actions):\n",
    "        board_sum = np.sum(self.game.board.board)\n",
    "        \n",
    "        #Monta um array de shape de mesma dimenção das ações\n",
    "        discounted_reward = np.zeros(np.array(actions).shape) \n",
    "        \n",
    "        if(board_sum > 0):\n",
    "            base_reward = 1*player\n",
    "        elif(board_sum < 0):\n",
    "            base_reward = -1*player\n",
    "        else:\n",
    "            base_reward = 0\n",
    "            return discounted_reward\n",
    "\n",
    "        #Captura ponto final para a ultima jogada do array\n",
    "        discounted_reward[-1] = base_reward\n",
    "\n",
    "        running_sum = 0\n",
    "        #Propaga o reward aplicando o desconto para todas as jogadas tomadas\n",
    "        for i in reversed(range(0, len(actions))):\n",
    "            running_sum = running_sum * self.gamma + discounted_reward[i]\n",
    "            discounted_reward[i] = running_sum\n",
    "        # normaliza as rewards\n",
    "        discounted_reward -= np.mean(discounted_reward)\n",
    "        discounted_reward /= np.std(discounted_reward)\n",
    "        \n",
    "        #retorna rewards por ações tomadas\n",
    "        return discounted_reward.tolist()\n",
    "    \n",
    "        #Testa rede contra um jogador aleatorio\n",
    "    def eval_player(self):\n",
    "        black_wins = 0\n",
    "        for _ in range(self.eval_test_size):\n",
    "            #Reinicia board ao estado inicial\n",
    "            self.game.board.reset()\n",
    "\n",
    "            #Printa jogo se verbose = true\n",
    "            self.v or print(self.game.board)\n",
    "            #Rede e jogador aleatorio tomam ações até q o jogo acabe\n",
    "            while(not self.game.board.finished()):\n",
    "                p1_move = self.game.player_move(self, self.game.BLACK)\n",
    "                if(p1_move != -1):\n",
    "                    self.v or print(self.game.board)\n",
    "                if (self.game.random_move(self.game.WHITE) != -1):\n",
    "                    self.v or print(self.game.board)\n",
    "            if(self.game.board.score()[0] >= self.game.board.score()[1]):\n",
    "                black_wins += 1 \n",
    "        #retorna pontuação\n",
    "        return black_wins\n",
    "    \n",
    "    def train(self):\n",
    "        #Inicializa pesos e variaveis\n",
    "        if(self.restore_file != \"\"):\n",
    "            #restaura pesos e variaveis\n",
    "            self.saver.restore(self.sess, self.restore_file)\n",
    "        else:\n",
    "            #Inicializa variaveis\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.statistics.start()\n",
    "       \n",
    "        #Add graph to tensorboard\n",
    "        self.statistics.writer.add_graph(self.sess.graph)\n",
    "\n",
    "        #Treina epochCount vezes\n",
    "        for j in range(self.epoch_count):           \n",
    "            #Adiciona states, actions e rewards ao replay_memory\n",
    "            self.add_samples_to_replay_mem()\n",
    "\n",
    "            #Recupera batchSize samples do replayMemory para o treinamento\n",
    "            boards, actions, rewards = self.sample_batch()\n",
    "\n",
    "            #Treina a rede\n",
    "            _, summary = self.sess.run([\n",
    "                                            self.graph[\"train_step\"],\n",
    "                                            self.statistics.merged_summary\n",
    "                                        ], \n",
    "                                        feed_dict={\n",
    "                                            self.graph[\"input_states\"]: boards, \n",
    "                                            self.graph[\"actions\"]: actions, \n",
    "                                            self.graph[\"rewards\"]: rewards,\n",
    "                                            self.graph[\"eval\"]: [self.eval_player()]\n",
    "                                        }\n",
    "            )\n",
    "            \n",
    "            #Escreve dados para o tensorBoard\n",
    "            self.statistics.writer.add_summary(summary)\n",
    "\n",
    "            #A cada batchSize epochs, salva os pesos da rede\n",
    "            if(j % self.batch_size == 0 and j != 0):\n",
    "                #Salva pesos\n",
    "                save_path = self.saver.save(self.sess, \"./pesos/pesos-\"+str(time.time())+\".ckpt\")\n",
    "                print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "        \n",
    "                \n",
    "\n",
    "brain = Brain(restore_file=\"\")\n",
    "brain.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
