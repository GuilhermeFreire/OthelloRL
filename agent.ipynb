{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ./pesos/pesos-1504819574.9374185.ckpt\n",
      "Loss: 0.145857\n",
      "Loss: 0.575047\n",
      "Loss: 0.189925\n",
      "Loss: 0.361393\n",
      "Loss: 0.203815\n",
      "Loss: 0.196249\n",
      "Loss: -0.967964\n",
      "Loss: -0.519173\n",
      "Loss: -3.25292\n",
      "Loss: -1.44447\n",
      "Loss: -7.44761\n",
      "Loss: -8.6772\n",
      "Loss: -19.7533\n",
      "Loss: -27.7486\n",
      "Loss: -15.3925\n",
      "Loss: -47.7877\n",
      "Loss: -44.2792\n",
      "Loss: -80.4518\n",
      "Loss: -193.538\n",
      "Loss: -208.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf #OLD VERSION ##########UPDATE THIS##########\n",
    "from othello import Board\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "def reset_graph():\n",
    "    if(\"sess\" in globals() and sess):\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "def buildGraph(inputDim, dataType, hLayersDim, learning_rate = 0.01, name=\"player\"):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.placeholder(dataType, [None, inputDim])\n",
    "        actions = tf.placeholder(tf.int32, [None])\n",
    "        rewards = tf.placeholder(dataType, [None])\n",
    "\n",
    "        layers = []\n",
    "        biases = []\n",
    "        preActivate = []\n",
    "\n",
    "        with tf.variable_scope(\"Weights\"):\n",
    "            layers.append(tf.get_variable(\"W0\", [inputDim, hLayersDim[0]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(layers[-1])\n",
    "            layers.append(tf.get_variable(\"W1\", [hLayersDim[0], hLayersDim[1]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(layers[-1])\n",
    "            layers.append(tf.get_variable(\"W2\", [hLayersDim[1], hLayersDim[2]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(layers[-1])\n",
    "            layers.append(tf.get_variable(\"W3\", [hLayersDim[2], inputDim], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(layers[-1])\n",
    "\n",
    "        with tf.variable_scope(\"Biases\"):\n",
    "            biases.append(tf.get_variable(\"b0\", [hLayersDim[0]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(biases[-1])\n",
    "            biases.append(tf.get_variable(\"b1\", [hLayersDim[1]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(biases[-1])\n",
    "            biases.append(tf.get_variable(\"b2\", [hLayersDim[2]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(biases[-1])\n",
    "            biases.append(tf.get_variable(\"b3\", [inputDim], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "            variable_summaries(biases[-1])\n",
    "\n",
    "        hiddenStates = []\n",
    "        \n",
    "        with tf.variable_scope(\"Activation\"):\n",
    "            hiddenStates.append(tf.nn.relu(tf.matmul(x, layers[0]) + biases[0])) #Olhar o relu6, pode ser melhor\n",
    "            tf.summary.histogram('hiddenStates', hiddenStates[-1])\n",
    "            hiddenStates.append(tf.nn.relu(tf.matmul(hiddenStates[-1], layers[1]) + biases[1])) #Olhar o relu6, pode ser melhor\n",
    "            tf.summary.histogram('hiddenStates', hiddenStates[-1])\n",
    "            hiddenStates.append(tf.nn.relu(tf.matmul(hiddenStates[-1], layers[2]) + biases[2])) #Olhar o relu6, pode ser melhor\n",
    "            tf.summary.histogram('hiddenStates', hiddenStates[-1])\n",
    "            hiddenStates.append(tf.matmul(hiddenStates[-1], layers[3]) + biases[3])\n",
    "            tf.summary.histogram('hiddenStates', hiddenStates[-1])\n",
    "\n",
    "        output_raw = hiddenStates[-1]\n",
    "        output = tf.nn.softmax(output_raw)\n",
    "\n",
    "        with tf.variable_scope(\"cross_entropy\"):\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output_raw, labels=actions)\n",
    "            loss = tf.reduce_mean(neg_log_prob * rewards)  # reward guided loss\n",
    "            tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"x\": x,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"negLogProb\": neg_log_prob,\n",
    "            \"loss\": loss,\n",
    "            \"layers\": layers,\n",
    "            \"biases\": biases,\n",
    "            \"hiddenStates\": hiddenStates,\n",
    "            \"output\": output,\n",
    "            \"outputRaw\": output_raw,\n",
    "            \"train_step\": train_step\n",
    "        }\n",
    "    \n",
    "def predict(agent, state):\n",
    "    return sess.run(agent[\"output\"], feed_dict={agent[\"x\"]: state})\n",
    "\n",
    "def makeMove(agent, board, name):\n",
    "    if(name == \"BLACK\"):\n",
    "        player = 1\n",
    "        tempBoard = board.board\n",
    "    else:\n",
    "        player = -1\n",
    "        tempBoard = board.inverted_board()\n",
    "       \n",
    "    moves = board.possible_moves(player) \n",
    "    if(len(moves) == 0):\n",
    "        return (-1, player)\n",
    "    probs = predict(agent, tempBoard.reshape([1,-1])).squeeze()\n",
    "    \n",
    "    new_probs = np.zeros(boardDim)\n",
    "    for x, y, _ in moves:\n",
    "        new_probs[x*8 + y] = probs[x*8 + y]\n",
    "    \n",
    "    if(np.sum(new_probs) == 0):\n",
    "        selected_move = np.random.choice(len(moves), 1, p = [1/len(moves)]*len(moves))[0]\n",
    "        selected_move = moves[selected_move]\n",
    "        selected_move = selected_move[0]*8 + selected_move[1]\n",
    "    else:\n",
    "        new_probs = new_probs/np.sum(new_probs)\n",
    "        selected_move = np.random.choice(boardDim, 1, p=new_probs)[0]\n",
    "    \n",
    "    board.move(selected_move//8, selected_move%8, player)\n",
    "    return (selected_move, player)\n",
    "        \n",
    "def maybePrint(shouldI, s):\n",
    "    if(shouldI):\n",
    "        print(s)\n",
    "\n",
    "def discount_and_norm_rewards(winLoseReward, actions, gamma):\n",
    "    # discount episode rewards\n",
    "    discounted_ep_rs = np.zeros(np.array(actions).shape)    \n",
    "    discounted_ep_rs[-1] = winLoseReward\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(actions))):\n",
    "        running_add = running_add * gamma + discounted_ep_rs[t]\n",
    "        discounted_ep_rs[t] = running_add\n",
    "\n",
    "    # normalize episode rewards\n",
    "    discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "    discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "    return discounted_ep_rs\n",
    "    \n",
    "def playGame(board, gamma, v = False):\n",
    "    board.reset()\n",
    "    \n",
    "    boardHistoryP1 = []\n",
    "    boardHistoryP2 = []\n",
    "    moveSequenceP1 = []\n",
    "    moveSequenceP2 = []\n",
    "    \n",
    "    maybePrint(v, b)\n",
    "    while(not b.finished()):\n",
    "        boardBeforeMoveP1 = deepcopy(board.board)\n",
    "        P1Move = makeMove(p, b, \"BLACK\")[0]\n",
    "        if(P1Move != -1):\n",
    "            boardHistoryP1.append(boardBeforeMoveP1)\n",
    "            moveSequenceP1.append(P1Move)\n",
    "        \n",
    "        boardBeforeMoveP2 = deepcopy(board.inverted_board())\n",
    "        P2Move = makeMove(p, b, \"WHITE\")[0]\n",
    "        if(P2Move != -1):\n",
    "            boardHistoryP2.append(boardBeforeMoveP2)\n",
    "            moveSequenceP2.append(P2Move)\n",
    "        maybePrint(v, \"====================\")\n",
    "        maybePrint(v, (moveSequenceP1[-1]//8, moveSequenceP1[-1]%8))\n",
    "        maybePrint(v, b)\n",
    "        maybePrint(v, \"====================\")\n",
    "        maybePrint(v, (moveSequenceP2[-1]//8, moveSequenceP2[-1]%8))\n",
    "        maybePrint(v, b)\n",
    "    maybePrint(v, b.score())\n",
    "    \n",
    "    r = winLoseReward(board)\n",
    "    rewardSequenceP1 = list(discount_and_norm_rewards(r if r != 0 else -1, moveSequenceP1, gamma))\n",
    "    rewardSequenceP2 = list(discount_and_norm_rewards(-r if r != 0 else -1, moveSequenceP2, gamma))\n",
    "\n",
    "    boardHistory = boardHistoryP1 + boardHistoryP2\n",
    "    moveSequence = moveSequenceP1 + moveSequenceP2\n",
    "    rewardSequence = rewardSequenceP1 + rewardSequenceP2\n",
    "    \n",
    "    return (boardHistory, moveSequence, rewardSequence)\n",
    "    \n",
    "def winLoseReward(board):\n",
    "    if(np.sum(board.board) > 0):\n",
    "        return 1\n",
    "    elif(np.sum(board.board) < 0):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def sampleBatch(mem, size):\n",
    "    indexes = []\n",
    "    while(len(indexes) < size):\n",
    "        i = np.random.randint(0,len(mem[\"states\"]))\n",
    "        if i not in indexes:\n",
    "            indexes.append(i)\n",
    "            \n",
    "    boards = [mem[\"states\"][i]for i in indexes]\n",
    "    actions = [mem[\"actions\"][i]for i in indexes]\n",
    "    rewards = [mem[\"rewards\"][i]for i in indexes]\n",
    "    return np.array(boards).reshape((size, boardDim)), actions, np.array(rewards)\n",
    "\n",
    "def randomPlay(board, name):\n",
    "    if(name == \"BLACK\"):\n",
    "        player = 1\n",
    "    else:\n",
    "        player = -1\n",
    "       \n",
    "    moves = board.possible_moves(player) \n",
    "    \n",
    "    if(len(moves) == 0):\n",
    "        return -1\n",
    "    \n",
    "    selected_move = np.random.choice(len(moves), 1, p = [1/len(moves)]*len(moves))[0]\n",
    "    board.move(moves[selected_move][0], moves[selected_move][1], player)        \n",
    "    \n",
    "def testPlayer(board, v = False):\n",
    "    board.reset()\n",
    "    \n",
    "    maybePrint(v, board)\n",
    "    while(not b.finished()):\n",
    "        P1Move = makeMove(p, b, \"BLACK\")[0]\n",
    "        if(P1Move != -1):\n",
    "            maybePrint(v, board)\n",
    "        if (randomPlay(b, \"WHITE\") != -1):\n",
    "            maybePrint(v, board)\n",
    "    return b.score()\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "batchSize = 128\n",
    "epochs = 50\n",
    "gamma = 0.95\n",
    "boardDim = 8*8\n",
    "hLayersDim = [128, 256, 128]\n",
    "gpuNum = 1\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "b = Board()\n",
    "p = buildGraph(boardDim, tf.float32, hLayersDim)\n",
    "replayMemory = {\n",
    "    \"states\": [],\n",
    "    \"actions\": [],\n",
    "    \"rewards\": []\n",
    "}\n",
    "losses = []\n",
    "testGames = []\n",
    "summaryCounter = 0\n",
    "\n",
    "#with tf.device('/gpu:'+str(gpuNum)):\n",
    "with tf.Session(config=config) as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    data_writer = tf.summary.FileWriter('./data')\n",
    "\n",
    "    for j in range(epochs):\n",
    "        for i in range(batchSize):\n",
    "            boards, moves, rewards = playGame(b, gamma)\n",
    "\n",
    "            replayMemory[\"states\"].extend(boards)\n",
    "            replayMemory[\"actions\"].extend(moves)\n",
    "            replayMemory[\"rewards\"].extend(rewards)\n",
    "\n",
    "        boards, actions, rewards = sampleBatch(replayMemory, batchSize)\n",
    "        summary, outputRaw, negLogProb, loss, _ = sess.run([merged, p[\"outputRaw\"], p[\"negLogProb\"], p[\"loss\"], p[\"train_step\"]], feed_dict={p[\"x\"]:boards, p[\"actions\"]: actions, p[\"rewards\"]: rewards})\n",
    "        data_writer.add_summary(summary, summaryCounter)\n",
    "        summaryCounter += 1\n",
    "        if(j % 100 == 0):\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess, \"./pesos/pesos-\"+str(time.time())+\".ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        losses.append(loss)\n",
    "        print(\"Loss: \"+str(loss))\n",
    "\n",
    "        black_wins = 0\n",
    "        for _ in range(100):\n",
    "            score = testPlayer(b)\n",
    "            if(score[0] >= score[1]):\n",
    "                black_wins += 1\n",
    "        testGames.append(black_wins)\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(testGames)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.average(testGames))\n",
    "print(np.average(losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
