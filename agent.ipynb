{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf #OLD VERSION ##########UPDATE THIS##########\n",
    "from othello import Board\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reset_graph():\n",
    "    if(\"sess\" in globals() and sess):\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def buildGraph(inputDim, dataType, hLayersDim, learning_rate = 5e-4, name=\"player\"):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        x = tf.placeholder(dataType, [None, inputDim])\n",
    "        actions = tf.placeholder(dataType, [None, inputDim])\n",
    "        rewards = tf.placeholder(dataType, [None])\n",
    "\n",
    "        layers = []\n",
    "        biases = []\n",
    "\n",
    "        layers.append(tf.get_variable(\"W0\", [inputDim, hLayersDim[0]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        layers.append(tf.get_variable(\"W1\", [hLayersDim[0], hLayersDim[1]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        layers.append(tf.get_variable(\"W2\", [hLayersDim[1], hLayersDim[2]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        layers.append(tf.get_variable(\"W3\", [hLayersDim[2], inputDim], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "\n",
    "        biases.append(tf.get_variable(\"b0\", [hLayersDim[0]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        biases.append(tf.get_variable(\"b1\", [hLayersDim[1]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        biases.append(tf.get_variable(\"b2\", [hLayersDim[2]], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        biases.append(tf.get_variable(\"b3\", [inputDim], dataType, initializer=tf.contrib.layers.xavier_initializer()))\n",
    "\n",
    "        hiddenStates = []\n",
    "\n",
    "        hiddenStates.append(tf.nn.tanh(tf.matmul(x, layers[0]) + biases[0])) #Olhar o relu6, pode ser melhor\n",
    "        hiddenStates.append(tf.nn.tanh(tf.matmul(hiddenStates[-1], layers[1]) + biases[1])) #Olhar o relu6, pode ser melhor\n",
    "        hiddenStates.append(tf.nn.tanh(tf.matmul(hiddenStates[-1], layers[2]) + biases[2])) #Olhar o relu6, pode ser melhor\n",
    "        hiddenStates.append(tf.matmul(hiddenStates[-1], layers[3]) + biases[3]) #Olhar o relu6, pode ser melhor\n",
    "\n",
    "        output_raw = hiddenStates[-1] #sparse_softmax_cross_entropy_with_logits\n",
    "        output = tf.nn.softmax(output_raw)\n",
    "\n",
    "        loss = tf.losses.softmax_cross_entropy(actions, output_raw, rewards)\n",
    "        \n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"x\": x,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"loss\": loss,\n",
    "            \"layers\": layers,\n",
    "            \"biases\": biases,\n",
    "            \"hiddenStates\": hiddenStates,\n",
    "            \"output\": output,\n",
    "            \"train_step\": train_step\n",
    "        }\n",
    "    \n",
    "def predict(agent, state):\n",
    "    return sess.run(agent[\"output\"], feed_dict={agent[\"x\"]: state})\n",
    "\n",
    "def makeMove(agent, board, name):\n",
    "    if(name == \"BLACK\"):\n",
    "        player = 1\n",
    "        tempBoard = board.board\n",
    "    else:\n",
    "        player = -1\n",
    "        tempBoard = board.inverted_board()\n",
    "       \n",
    "    moves = board.possible_moves(player) \n",
    "    if(len(moves) == 0):\n",
    "        return (-1, player)\n",
    "    probs = predict(agent, tempBoard.reshape([1,-1])).squeeze()\n",
    "    \n",
    "    new_probs = np.zeros(boardDim)\n",
    "    for x, y, _ in moves:\n",
    "        new_probs[x*8 + y] = probs[x*8 + y]\n",
    "    \n",
    "    if(np.sum(new_probs) == 0):\n",
    "        print(\"==================================\")\n",
    "        print(\"random move\")\n",
    "        selected_move = np.random.choice(len(moves), 1, p = [1/len(moves)]*len(moves))[0]\n",
    "        print(\"selected_move index\")\n",
    "        print(selected_move)\n",
    "        selected_move = moves[selected_move]\n",
    "        print(\"selected_move val\")\n",
    "        print(selected_move)\n",
    "        selected_move = selected_move[0]*8 + selected_move[1]\n",
    "        print(\"selected_move pos\")\n",
    "        print(selected_move)\n",
    "        print(\"board\")\n",
    "        print(board)\n",
    "        print(\"moves\")\n",
    "        print(moves)\n",
    "        print(\"probs\")\n",
    "        print(probs)\n",
    "        print(\"==================================\")\n",
    "    else:\n",
    "        new_probs = new_probs/np.sum(new_probs)\n",
    "        selected_move = np.random.choice(boardDim, 1, p=new_probs)[0]\n",
    "    \n",
    "    board.move(selected_move//8, selected_move%8, player)\n",
    "    \n",
    "    return (selected_move, player)\n",
    "        \n",
    "def maybePrint(shouldI, s):\n",
    "    if(shouldI):\n",
    "        print(s)\n",
    "    \n",
    "def playGame(board, gamma, v = False):\n",
    "    board.reset()\n",
    "    \n",
    "    boardHistoryP1 = []\n",
    "    boardHistoryP2 = []\n",
    "    moveSequenceP1 = []\n",
    "    moveSequenceP2 = []\n",
    "    \n",
    "    maybePrint(v, b)\n",
    "    while(not b.finished()):\n",
    "        boardBeforeMoveP1 = deepcopy(board.board)\n",
    "        P1Move = makeMove(p, b, \"BLACK\")[0]\n",
    "        if(P1Move != -1):\n",
    "            boardHistoryP1.append(boardBeforeMoveP1)\n",
    "            moveSequenceP1.append(P1Move)\n",
    "        \n",
    "        boardBeforeMoveP2 = deepcopy(board.inverted_board())\n",
    "        P2Move = makeMove(p, b, \"WHITE\")[0]\n",
    "        if(P2Move != -1):\n",
    "            boardHistoryP2.append(boardBeforeMoveP2)\n",
    "            moveSequenceP2.append(P1Move)\n",
    "        maybePrint(v, \"====================\")\n",
    "        maybePrint(v, (moveSequenceP1[-1]//8, moveSequenceP1[-1]%8))\n",
    "        maybePrint(v, b)\n",
    "        maybePrint(v, \"====================\")\n",
    "        maybePrint(v, (moveSequenceP2[-1]//8, moveSequenceP2[-1]%8))\n",
    "        maybePrint(v, b)\n",
    "    maybePrint(v, b.score())\n",
    "    \n",
    "    rewardSequenceP1 = []\n",
    "    rewardSequenceP2 = []\n",
    "    r = reward(board)\n",
    "    \n",
    "    for i in range(len(moveSequenceP1)):\n",
    "        rewardSequenceP1.append(-r* (gamma**i))\n",
    "    \n",
    "    for i in range(len(moveSequenceP2)):\n",
    "        rewardSequenceP2.append(r* (gamma**i))\n",
    "        \n",
    "    rewardSequenceP1.reverse()\n",
    "    rewardSequenceP2.reverse()\n",
    "    boardHistory = boardHistoryP1 + boardHistoryP2\n",
    "    moveSequence = moveSequenceP1 + moveSequenceP2\n",
    "    rewardSequence = rewardSequenceP1 + rewardSequenceP2\n",
    "    \n",
    "    return (boardHistory, moveSequence, rewardSequence)\n",
    "    \n",
    "def reward(board):\n",
    "    if(np.sum(board.board) > 0):\n",
    "        return 1\n",
    "    elif(np.sum(board.board) < 0):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def idx2onehot(idx):\n",
    "    onehots = np.zeros((len(idx), boardDim))\n",
    "    count = 0\n",
    "    for i in idx:\n",
    "        onehots[count][i] = 1\n",
    "        count+=1\n",
    "    return onehots\n",
    "    \n",
    "def sampleBatch(mem, size):\n",
    "    indexes = []\n",
    "    while(len(indexes) < size):\n",
    "        i = np.random.randint(0,len(mem[\"states\"]))\n",
    "        if i not in indexes:\n",
    "            indexes.append(i)\n",
    "            \n",
    "    boards = [mem[\"states\"][i]for i in indexes]\n",
    "    actions = idx2onehot([mem[\"actions\"][i]for i in indexes])\n",
    "    rewards = [mem[\"rewards\"][i]for i in indexes]\n",
    "    return np.array(boards).reshape((size, boardDim)), actions, np.array(rewards)\n",
    "\n",
    "def randomPlay(board, name):\n",
    "    if(name == \"BLACK\"):\n",
    "        player = 1\n",
    "    else:\n",
    "        player = -1\n",
    "       \n",
    "    moves = board.possible_moves(player) \n",
    "    \n",
    "    if(len(moves) == 0):\n",
    "        return -1\n",
    "    \n",
    "    selected_move = np.random.choice(len(moves), 1, p = [1/len(moves)]*len(moves))[0]\n",
    "    board.move(moves[selected_move][0], moves[selected_move][1], player)        \n",
    "    \n",
    "def testPlayer(board, v = False):\n",
    "    board.reset()\n",
    "    \n",
    "    maybePrint(v, board)\n",
    "    while(not b.finished()):\n",
    "        P1Move = makeMove(p, b, \"BLACK\")[0]\n",
    "        if(P1Move != -1):\n",
    "            maybePrint(v, board)\n",
    "        if (randomPlay(b, \"WHITE\") != -1):\n",
    "            maybePrint(v, board)\n",
    "    return b.score()\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "batchSize = 32\n",
    "epochs = 1000\n",
    "gamma = 0.99\n",
    "boardDim = 8*8\n",
    "hLayersDim = [128, 256, 128]\n",
    "\n",
    "b = Board()\n",
    "p = buildGraph(boardDim, tf.float32, hLayersDim)\n",
    "replayMemory = {\n",
    "    \"states\": [],\n",
    "    \"actions\": [],\n",
    "    \"rewards\": []\n",
    "}\n",
    "losses = []\n",
    "testGames = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        for i in range(batchSize):\n",
    "            boards, moves, rewards = playGame(b, gamma)\n",
    "            replayMemory[\"states\"].extend(boards)\n",
    "            replayMemory[\"actions\"].extend(moves)\n",
    "            replayMemory[\"rewards\"].extend(rewards)\n",
    "\n",
    "        boards, actions, rewards = sampleBatch(replayMemory, batchSize)\n",
    "        loss, _ = sess.run([p[\"loss\"], p[\"train_step\"]], feed_dict={p[\"x\"]:boards, p[\"actions\"]: actions, p[\"rewards\"]:rewards})\n",
    "        losses.append(loss)\n",
    "        print(\"Erro: \"+str(loss))\n",
    "        \n",
    "        black_wins = 0\n",
    "        for _ in range(100):\n",
    "            score = testPlayer(b)\n",
    "            if(score[0] >= score[1]):\n",
    "                black_wins += 1\n",
    "        testGames.append(black_wins)\n",
    "        \n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(testGames)\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
